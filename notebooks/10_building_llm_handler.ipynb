{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35306b3c",
   "metadata": {},
   "source": [
    "# 09.01 ‚Äì Building Our LLM Handler\n",
    "Let's bridge the gap between our rule-based chatbot and connecting to real LLMs by building our own handler step-by-step! üî®\n",
    "\n",
    "## ‚úÖ Goals\n",
    "- Understand what an LLM handler does\n",
    "- Build a handler class from scratch\n",
    "- Add support for multiple LLM providers\n",
    "- Implement conversation history tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e1aa4",
   "metadata": {},
   "source": [
    "## ü§î Why Do We Need an LLM Handler?\n",
    "\n",
    "Our rule-based chatbot from the previous notebook was limited to pre-defined responses. To create a more dynamic and intelligent chatbot, we need to connect to Large Language Models (LLMs).\n",
    "\n",
    "An LLM handler helps us:\n",
    "1. Connect to different LLM providers (like OpenAI or local models)\n",
    "2. Format prompts correctly\n",
    "3. Track conversation history\n",
    "4. Handle API calls and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d630f7",
   "metadata": {},
   "source": [
    "## üîç Step 1: Setting Up the Environment\n",
    "\n",
    "First, let's import the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # This will load environment variables from a .env file\n",
    "\n",
    "# Test if our environment is set up correctly\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d630f7",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 2: Creating the Basic Handler Class\n",
    "\n",
    "Let's start with a simple version of our LLM handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLLMHandler:\n",
    "    def __init__(self, name=\"PyPal\", role=\"helpful AI assistant\"):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        # For now, just return a mock response\n",
    "        return f\"[{self.name}]: I am a {self.role} and you said: {prompt}\"\n",
    "\n",
    "# Test our basic handler\n",
    "basic_handler = BasicLLMHandler(name=\"Buddy\", role=\"Python tutor\")\n",
    "response = basic_handler.generate(\"Hello, can you help me learn Python?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d630f7",
   "metadata": {},
   "source": [
    "## üß† Step 3: Adding Conversation History\n",
    "\n",
    "Let's enhance our handler to keep track of the conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMHandlerWithHistory:\n",
    "    def __init__(self, name=\"PyPal\", role=\"helpful AI assistant\"):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "        self.history = []\n",
    "    \n",
    "    def add_to_history(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Display the conversation history.\"\"\"\n",
    "        for entry in self.history:\n",
    "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response to the given prompt.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.add_to_history(\"user\", prompt)\n",
    "        \n",
    "        # For now, just return a mock response\n",
    "        response = f\"I am {self.name} the {self.role}. You said: {prompt}\"\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.add_to_history(\"assistant\", response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test our handler with history\n",
    "history_handler = LLMHandlerWithHistory(name=\"Alex\", role=\"coding coach\")\n",
    "print(history_handler.generate(\"Hi there!\"))\n",
    "print(history_handler.generate(\"Can you help me with Python?\"))\n",
    "print(\"\\nConversation History:\")\n",
    "history_handler.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1d630f7",
   "metadata": {},
   "source": [
    "## üîå Step 4: Connecting to OpenAI\n",
    "\n",
    "Now let's add functionality to connect to OpenAI's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIHandler:\n",
    "    def __init__(self, model=\"gpt-4o-mini\", name=\"PyPal\", role=\"helpful AI assistant\"):\n",
    "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n",
    "            \n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "        self.history = []\n",
    "    \n",
    "    def add_to_history(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def show_history(self):\n",
    "        for entry in self.history:\n",
    "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "\n",
    "# Note: We won't run this yet because it requires a valid API key\n",
    "# openai_handler = OpenAIHandler()\n",
    "# print(openai_handler.generate(\"Hello, how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1d630f7",
   "metadata": {},
   "source": [
    "## üåê Step 5: Supporting Local Models with Ollama\n",
    "\n",
    "Let's add support for local models using Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaHandler:\n",
    "    def __init__(self, model=\"llama3\", name=\"PyPal\", role=\"helpful AI assistant\"):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "        self.history = []\n",
    "    \n",
    "    def add_to_history(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def show_history(self):\n",
    "        for entry in self.history:\n",
    "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response using Ollama's API.\"\"\"\n",
    "        url = \"http://localhost:11434/api/chat\"\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False  # We don't want streaming for now\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result.get(\"message\", {}).get(\"content\", \"No reply\")\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "\n",
    "# Note: We won't run this yet because it requires Ollama to be running\n",
    "# ollama_handler = OllamaHandler()\n",
    "# print(ollama_handler.generate(\"Hello, how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1d630f7",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Creating a Unified Handler\n",
    "\n",
    "Now, let's combine everything into a unified handler that can work with both OpenAI and Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedLLMHandler:\n",
    "    def __init__(self, provider=\"openai\", model=\"gpt-4o-mini\", name=\"PyPal\", role=\"helpful AI assistant\"):\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "        self.history = []\n",
    "        \n",
    "        # Check for API key if using OpenAI\n",
    "        if self.provider == \"openai\":\n",
    "            self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not self.api_key:\n",
    "                raise ValueError(\"OPENAI_API_KEY environment variable is required for OpenAI\")\n",
    "    \n",
    "    def add_to_history(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Display the conversation history.\"\"\"\n",
    "        for entry in self.history:\n",
    "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate a response based on the configured provider.\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            return self._chat_with_openai(prompt)\n",
    "        elif self.provider == \"ollama\":\n",
    "            return self._chat_with_ollama(prompt)\n",
    "        else:\n",
    "            # Mock provider for testing\n",
    "            mock_response = f\"[MOCK {self.name}]: As a {self.role}, I respond to: {prompt}\"\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", mock_response)\n",
    "            return mock_response\n",
    "    \n",
    "    def _chat_with_openai(self, prompt):\n",
    "        \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with OpenAI: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "    \n",
    "    def _chat_with_ollama(self, prompt):\n",
    "        \"\"\"Generate a response using Ollama's API.\"\"\"\n",
    "        url = \"http://localhost:11434/api/chat\"\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result.get(\"message\", {}).get(\"content\", \"No reply\")\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with Ollama: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "\n",
    "# Test with mock provider (no real API calls)\n",
    "handler = UnifiedLLMHandler(provider=\"mock\", name=\"Tester\", role=\"debugging assistant\")\n",
    "print(handler.generate(\"Hello, can you help me debug my code?\"))\n",
    "print(handler.generate(\"I'm getting an IndexError.\"))\n",
    "print(\"\\nConversation History:\")\n",
    "handler.show_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1d630f7",
   "metadata": {},
   "source": [
    "## üåü Step 7: Creating a Simple Interface\n",
    "\n",
    "Let's create a simple chat interface using our unified handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_llm(handler):\n",
    "    print(f\"Chatting with {handler.name} the {handler.role}\")\n",
    "    print(\"Type 'bye' to exit, 'history' to see conversation history\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() in [\"bye\", \"exit\", \"quit\"]:\n",
    "            print(f\"{handler.name}: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        elif user_input.lower() == \"history\":\n",
    "            print(\"\\nConversation History:\")\n",
    "            handler.show_history()\n",
    "            print()  # Empty line for better formatting\n",
    "            continue\n",
    "        \n",
    "        # Generate response\n",
    "        response = handler.generate(user_input)\n",
    "        print(f\"{handler.name}: {response}\")\n",
    "\n",
    "# We can run this with our mock handler\n",
    "mock_handler = UnifiedLLMHandler(provider=\"mock\", name=\"MockBot\", role=\"coding assistant\")\n",
    "# Uncomment to test the chat interface:\n",
    "# chat_with_llm(mock_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1d630f7",
   "metadata": {},
   "source": [
    "## üîç Step 8: Refining Our Handler for Type Hints\n",
    "\n",
    "Let's add type hints to our unified handler for better code quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLMHandler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        provider: str = \"openai\",\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        name: str = \"PyPal\",\n",
    "        role: str = \"helpful AI assistant\",\n",
    "    ) -> None:\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.system_prompt = f\"Your name is {name}, and you are a {role}. Always respond accordingly to this role.\"\n",
    "        self.history = []\n",
    "        \n",
    "        # Check for API key if using OpenAI\n",
    "        if self.provider == \"openai\":\n",
    "            self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not self.api_key:\n",
    "                raise ValueError(\"OPENAI_API_KEY environment variable is required for OpenAI\")\n",
    "    \n",
    "    def add_to_history(self, role: str, content: str) -> None:\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def show_history(self) -> None:\n",
    "        \"\"\"Display the conversation history.\"\"\"\n",
    "        for entry in self.history:\n",
    "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a response based on the configured provider.\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            return self._chat_with_openai(prompt)\n",
    "        elif self.provider == \"ollama\":\n",
    "            return self._chat_with_ollama(prompt)\n",
    "        else:\n",
    "            # Mock provider for testing\n",
    "            mock_response = f\"[MOCK {self.name}]: As a {self.role}, I respond to: {prompt}\"\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", mock_response)\n",
    "            return mock_response\n",
    "    \n",
    "    def _chat_with_openai(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
    "        url = \"https://api.openai.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with OpenAI: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "    \n",
    "    def _chat_with_ollama(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a response using Ollama's API.\"\"\"\n",
    "        url = \"http://localhost:11434/api/chat\"\n",
    "        \n",
    "        # Get recent history (last 3 messages)\n",
    "        recent = self.history[-3:] if len(self.history) >= 3 else self.history[:]\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation history\n",
    "        messages.extend(recent)\n",
    "        \n",
    "        # Add current user message\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Prepare the request data\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.post(url, json=data)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            \n",
    "            # Parse the response\n",
    "            result = response.json()\n",
    "            reply = result.get(\"message\", {}).get(\"content\", \"No reply\")\n",
    "            \n",
    "            # Add to history\n",
    "            self.add_to_history(\"user\", prompt)\n",
    "            self.add_to_history(\"assistant\", reply)\n",
    "            \n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with Ollama: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "\n",
    "# Test our final SimpleLLMHandler with the mock provider\n",
    "final_handler = SimpleLLMHandler(provider=\"mock\", name=\"FinalBot\", role=\"Python tutor\")\n",
    "print(final_handler.generate(\"Can you explain what our LLM handler does?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1d630f7",
   "metadata": {},
   "source": [
    "## üìù Exercise: Let's Practice!\n",
    "\n",
    "Now that we've built our LLM handler from scratch, let's practice using it. Try the following exercises:\n",
    "\n",
    "1. Create a `SimpleLLMHandler` with a custom name and role\n",
    "2. Generate responses to a few different prompts\n",
    "3. Check the conversation history\n",
    "4. If you have an OpenAI API key or Ollama running, try connecting to a real LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r1d630f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "# Example:\n",
    "# my_handler = SimpleLLMHandler(provider=\"mock\", name=\"MyBot\", role=\"Data Science Expert\")\n",
    "# print(my_handler.generate(\"Can you explain what a neural network is?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1d630f7",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "In this notebook, we've built an LLM handler from scratch with the following features:\n",
    "\n",
    "1. Support for multiple LLM providers (OpenAI and Ollama)\n",
    "2. Conversation history tracking\n",
    "3. System prompt customization\n",
    "4. Error handling\n",
    "5. Mock provider for testing\n",
    "\n",
    "In the next notebook, we'll use this handler to connect our chatbot to real LLMs and see how it performs! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
} 
